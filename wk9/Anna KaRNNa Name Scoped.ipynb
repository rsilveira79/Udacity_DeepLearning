{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([45,  1, 59, 12, 43, 15, 53, 32, 16, 58, 58, 58, 60, 59, 12, 12, 52,\n",
       "       32, 76, 59, 17, 29, 31, 29, 15, 48, 32, 59, 53, 15, 32, 59, 31, 31,\n",
       "       32, 59, 31, 29, 26, 15, 51, 32, 15, 41, 15, 53, 52, 32, 10, 62,  1,\n",
       "       59, 12, 12, 52, 32, 76, 59, 17, 29, 31, 52, 32, 29, 48, 32, 10, 62,\n",
       "        1, 59, 12, 12, 52, 32, 29, 62, 32, 29, 43, 48, 32, 67, 46, 62, 58,\n",
       "       46, 59, 52, 72, 58, 58, 44, 41, 15, 53, 52, 43,  1, 29, 62])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the number of batches. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[45,  1, 59, 12, 43, 15, 53, 32, 16, 58],\n",
       "       [56, 62, 77, 32,  1, 15, 32, 17, 67, 41],\n",
       "       [32, 33, 59, 43, 33,  1, 29, 62, 73, 32],\n",
       "       [67, 43,  1, 15, 53, 32, 46, 67, 10, 31],\n",
       "       [32, 43,  1, 15, 32, 31, 59, 62, 77, 18],\n",
       "       [32,  3,  1, 53, 67, 10, 73,  1, 32, 31],\n",
       "       [43, 32, 43, 67, 58, 77, 67, 72, 58, 58],\n",
       "       [67, 32,  1, 15, 53, 48, 15, 31, 76, 78],\n",
       "       [ 1, 59, 43, 32, 29, 48, 32, 43,  1, 15],\n",
       "       [15, 53, 48, 15, 31, 76, 32, 59, 62, 77]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll write another function to grab batches out of the arrays made by split data. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "        \n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "        x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "    \n",
    "    with tf.name_scope('targets'):\n",
    "        targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "        y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "        y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # Build the RNN layers\n",
    "    with tf.name_scope(\"RNN_layers\"):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    \n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
    "        outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
    "    \n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one row for each cell output\n",
    "    with tf.name_scope('sequence_reshape'):\n",
    "        seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "        output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Now connect the RNN putputs to a softmax layer and calculate the cost\n",
    "    with tf.name_scope('logits'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                               name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "\n",
    "    with tf.name_scope('predictions'):\n",
    "        preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    \n",
    "    with tf.name_scope('cost'):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "        cost = tf.reduce_mean(loss, name='cost')\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    with tf.name_scope('train'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. The two you probably haven't seen before are `lstm_size` and `num_layers`. These set the number of hidden units in the LSTM layers and the number of LSTM layers, respectively. Of course, making these bigger will improve the network's performance but you'll have to watch out for overfitting. If your validation loss is much larger than the training loss, you're probably overfitting. Decrease the size of the network or decrease the dropout keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out the graph for TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    file_writer = tf.summary.FileWriter('./logs/2', sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p checkpoints/anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10  Iteration 1/1780 Training loss: 4.4196 3.2380 sec/batch\n",
      "Epoch 1/10  Iteration 2/1780 Training loss: 4.3723 1.2448 sec/batch\n",
      "Epoch 1/10  Iteration 3/1780 Training loss: 4.1864 1.2464 sec/batch\n",
      "Epoch 1/10  Iteration 4/1780 Training loss: 4.1722 1.2414 sec/batch\n",
      "Epoch 1/10  Iteration 5/1780 Training loss: 4.0889 1.2439 sec/batch\n",
      "Epoch 1/10  Iteration 6/1780 Training loss: 4.0088 1.2259 sec/batch\n",
      "Epoch 1/10  Iteration 7/1780 Training loss: 3.9360 1.2499 sec/batch\n",
      "Epoch 1/10  Iteration 8/1780 Training loss: 3.8716 1.2703 sec/batch\n",
      "Epoch 1/10  Iteration 9/1780 Training loss: 3.8174 1.2392 sec/batch\n",
      "Epoch 1/10  Iteration 10/1780 Training loss: 3.7699 1.2370 sec/batch\n",
      "Epoch 1/10  Iteration 11/1780 Training loss: 3.7276 1.2333 sec/batch\n",
      "Epoch 1/10  Iteration 12/1780 Training loss: 3.6925 1.2402 sec/batch\n",
      "Epoch 1/10  Iteration 13/1780 Training loss: 3.6611 1.2392 sec/batch\n",
      "Epoch 1/10  Iteration 14/1780 Training loss: 3.6340 1.2523 sec/batch\n",
      "Epoch 1/10  Iteration 15/1780 Training loss: 3.6096 1.2496 sec/batch\n",
      "Epoch 1/10  Iteration 16/1780 Training loss: 3.5886 1.2345 sec/batch\n",
      "Epoch 1/10  Iteration 17/1780 Training loss: 3.5681 1.2868 sec/batch\n",
      "Epoch 1/10  Iteration 18/1780 Training loss: 3.5511 1.2747 sec/batch\n",
      "Epoch 1/10  Iteration 19/1780 Training loss: 3.5351 1.2391 sec/batch\n",
      "Epoch 1/10  Iteration 20/1780 Training loss: 3.5186 1.2452 sec/batch\n",
      "Epoch 1/10  Iteration 21/1780 Training loss: 3.5042 1.2722 sec/batch\n",
      "Epoch 1/10  Iteration 22/1780 Training loss: 3.4913 1.2449 sec/batch\n",
      "Epoch 1/10  Iteration 23/1780 Training loss: 3.4793 1.2487 sec/batch\n",
      "Epoch 1/10  Iteration 24/1780 Training loss: 3.4680 1.2702 sec/batch\n",
      "Epoch 1/10  Iteration 25/1780 Training loss: 3.4571 1.2609 sec/batch\n",
      "Epoch 1/10  Iteration 26/1780 Training loss: 3.4479 1.2659 sec/batch\n",
      "Epoch 1/10  Iteration 27/1780 Training loss: 3.4393 1.2594 sec/batch\n",
      "Epoch 1/10  Iteration 28/1780 Training loss: 3.4303 1.2430 sec/batch\n",
      "Epoch 1/10  Iteration 29/1780 Training loss: 3.4221 1.2409 sec/batch\n",
      "Epoch 1/10  Iteration 30/1780 Training loss: 3.4147 1.2471 sec/batch\n",
      "Epoch 1/10  Iteration 31/1780 Training loss: 3.4082 1.2464 sec/batch\n",
      "Epoch 1/10  Iteration 32/1780 Training loss: 3.4011 1.2720 sec/batch\n",
      "Epoch 1/10  Iteration 33/1780 Training loss: 3.3944 1.3645 sec/batch\n",
      "Epoch 1/10  Iteration 34/1780 Training loss: 3.3885 1.2719 sec/batch\n",
      "Epoch 1/10  Iteration 35/1780 Training loss: 3.3824 1.2554 sec/batch\n",
      "Epoch 1/10  Iteration 36/1780 Training loss: 3.3769 1.2359 sec/batch\n",
      "Epoch 1/10  Iteration 37/1780 Training loss: 3.3710 1.2479 sec/batch\n",
      "Epoch 1/10  Iteration 38/1780 Training loss: 3.3654 1.2349 sec/batch\n",
      "Epoch 1/10  Iteration 39/1780 Training loss: 3.3598 1.2584 sec/batch\n",
      "Epoch 1/10  Iteration 40/1780 Training loss: 3.3548 1.2569 sec/batch\n",
      "Epoch 1/10  Iteration 41/1780 Training loss: 3.3499 1.2538 sec/batch\n",
      "Epoch 1/10  Iteration 42/1780 Training loss: 3.3454 1.2523 sec/batch\n",
      "Epoch 1/10  Iteration 43/1780 Training loss: 3.3409 1.2416 sec/batch\n",
      "Epoch 1/10  Iteration 44/1780 Training loss: 3.3365 1.2564 sec/batch\n",
      "Epoch 1/10  Iteration 45/1780 Training loss: 3.3322 1.2466 sec/batch\n",
      "Epoch 1/10  Iteration 46/1780 Training loss: 3.3284 1.2675 sec/batch\n",
      "Epoch 1/10  Iteration 47/1780 Training loss: 3.3248 1.2519 sec/batch\n",
      "Epoch 1/10  Iteration 48/1780 Training loss: 3.3214 1.2764 sec/batch\n",
      "Epoch 1/10  Iteration 49/1780 Training loss: 3.3182 1.2939 sec/batch\n",
      "Epoch 1/10  Iteration 50/1780 Training loss: 3.3149 1.2782 sec/batch\n",
      "Epoch 1/10  Iteration 51/1780 Training loss: 3.3118 1.2664 sec/batch\n",
      "Epoch 1/10  Iteration 52/1780 Training loss: 3.3086 1.2644 sec/batch\n",
      "Epoch 1/10  Iteration 53/1780 Training loss: 3.3057 1.2885 sec/batch\n",
      "Epoch 1/10  Iteration 54/1780 Training loss: 3.3026 1.2689 sec/batch\n",
      "Epoch 1/10  Iteration 55/1780 Training loss: 3.2999 1.2831 sec/batch\n",
      "Epoch 1/10  Iteration 56/1780 Training loss: 3.2969 1.2726 sec/batch\n",
      "Epoch 1/10  Iteration 57/1780 Training loss: 3.2942 1.2740 sec/batch\n",
      "Epoch 1/10  Iteration 58/1780 Training loss: 3.2916 1.2658 sec/batch\n",
      "Epoch 1/10  Iteration 59/1780 Training loss: 3.2888 1.3211 sec/batch\n",
      "Epoch 1/10  Iteration 60/1780 Training loss: 3.2863 1.2862 sec/batch\n",
      "Epoch 1/10  Iteration 61/1780 Training loss: 3.2840 1.2615 sec/batch\n",
      "Epoch 1/10  Iteration 62/1780 Training loss: 3.2819 1.2700 sec/batch\n",
      "Epoch 1/10  Iteration 63/1780 Training loss: 3.2799 1.2664 sec/batch\n",
      "Epoch 1/10  Iteration 64/1780 Training loss: 3.2773 1.2643 sec/batch\n",
      "Epoch 1/10  Iteration 65/1780 Training loss: 3.2749 1.3085 sec/batch\n",
      "Epoch 1/10  Iteration 66/1780 Training loss: 3.2729 1.3169 sec/batch\n",
      "Epoch 1/10  Iteration 67/1780 Training loss: 3.2709 1.2750 sec/batch\n",
      "Epoch 1/10  Iteration 68/1780 Training loss: 3.2683 1.2711 sec/batch\n",
      "Epoch 1/10  Iteration 69/1780 Training loss: 3.2660 1.2688 sec/batch\n",
      "Epoch 1/10  Iteration 70/1780 Training loss: 3.2641 1.2580 sec/batch\n",
      "Epoch 1/10  Iteration 71/1780 Training loss: 3.2621 1.2565 sec/batch\n",
      "Epoch 1/10  Iteration 72/1780 Training loss: 3.2604 1.2804 sec/batch\n",
      "Epoch 1/10  Iteration 73/1780 Training loss: 3.2585 1.2780 sec/batch\n",
      "Epoch 1/10  Iteration 74/1780 Training loss: 3.2566 1.2748 sec/batch\n",
      "Epoch 1/10  Iteration 75/1780 Training loss: 3.2549 1.2740 sec/batch\n",
      "Epoch 1/10  Iteration 76/1780 Training loss: 3.2533 1.2577 sec/batch\n",
      "Epoch 1/10  Iteration 77/1780 Training loss: 3.2514 1.2737 sec/batch\n",
      "Epoch 1/10  Iteration 78/1780 Training loss: 3.2497 1.2597 sec/batch\n",
      "Epoch 1/10  Iteration 79/1780 Training loss: 3.2478 1.2800 sec/batch\n",
      "Epoch 1/10  Iteration 80/1780 Training loss: 3.2459 1.2798 sec/batch\n",
      "Epoch 1/10  Iteration 81/1780 Training loss: 3.2440 1.2624 sec/batch\n",
      "Epoch 1/10  Iteration 82/1780 Training loss: 3.2423 1.2810 sec/batch\n",
      "Epoch 1/10  Iteration 83/1780 Training loss: 3.2407 1.2819 sec/batch\n",
      "Epoch 1/10  Iteration 84/1780 Training loss: 3.2389 1.2975 sec/batch\n",
      "Epoch 1/10  Iteration 85/1780 Training loss: 3.2370 1.2649 sec/batch\n",
      "Epoch 1/10  Iteration 86/1780 Training loss: 3.2353 1.3095 sec/batch\n",
      "Epoch 1/10  Iteration 87/1780 Training loss: 3.2335 1.3114 sec/batch\n",
      "Epoch 1/10  Iteration 88/1780 Training loss: 3.2317 1.2475 sec/batch\n",
      "Epoch 1/10  Iteration 89/1780 Training loss: 3.2301 1.2439 sec/batch\n",
      "Epoch 1/10  Iteration 90/1780 Training loss: 3.2285 1.2565 sec/batch\n",
      "Epoch 1/10  Iteration 91/1780 Training loss: 3.2269 1.2464 sec/batch\n",
      "Epoch 1/10  Iteration 92/1780 Training loss: 3.2252 1.2617 sec/batch\n",
      "Epoch 1/10  Iteration 93/1780 Training loss: 3.2235 1.2799 sec/batch\n",
      "Epoch 1/10  Iteration 94/1780 Training loss: 3.2218 1.2719 sec/batch\n",
      "Epoch 1/10  Iteration 95/1780 Training loss: 3.2200 1.2909 sec/batch\n",
      "Epoch 1/10  Iteration 96/1780 Training loss: 3.2181 1.2591 sec/batch\n",
      "Epoch 1/10  Iteration 97/1780 Training loss: 3.2164 1.2487 sec/batch\n",
      "Epoch 1/10  Iteration 98/1780 Training loss: 3.2146 1.2497 sec/batch\n",
      "Epoch 1/10  Iteration 99/1780 Training loss: 3.2127 1.2800 sec/batch\n",
      "Epoch 1/10  Iteration 100/1780 Training loss: 3.2110 1.2790 sec/batch\n",
      "Epoch 1/10  Iteration 101/1780 Training loss: 3.2093 1.2676 sec/batch\n",
      "Epoch 1/10  Iteration 102/1780 Training loss: 3.2077 1.2814 sec/batch\n",
      "Epoch 1/10  Iteration 103/1780 Training loss: 3.2060 1.2679 sec/batch\n",
      "Epoch 1/10  Iteration 104/1780 Training loss: 3.2042 1.2786 sec/batch\n",
      "Epoch 1/10  Iteration 105/1780 Training loss: 3.2023 1.2800 sec/batch\n",
      "Epoch 1/10  Iteration 106/1780 Training loss: 3.2005 1.2740 sec/batch\n",
      "Epoch 1/10  Iteration 107/1780 Training loss: 3.1986 1.2756 sec/batch\n",
      "Epoch 1/10  Iteration 108/1780 Training loss: 3.1967 1.2889 sec/batch\n",
      "Epoch 1/10  Iteration 109/1780 Training loss: 3.1948 1.2978 sec/batch\n",
      "Epoch 1/10  Iteration 110/1780 Training loss: 3.1925 1.2954 sec/batch\n",
      "Epoch 1/10  Iteration 111/1780 Training loss: 3.1903 1.2874 sec/batch\n",
      "Epoch 1/10  Iteration 112/1780 Training loss: 3.1882 1.2779 sec/batch\n",
      "Epoch 1/10  Iteration 113/1780 Training loss: 3.1859 1.2667 sec/batch\n",
      "Epoch 1/10  Iteration 114/1780 Training loss: 3.1844 1.2954 sec/batch\n",
      "Epoch 1/10  Iteration 115/1780 Training loss: 3.1825 1.2578 sec/batch\n",
      "Epoch 1/10  Iteration 116/1780 Training loss: 3.1806 1.2530 sec/batch\n",
      "Epoch 1/10  Iteration 117/1780 Training loss: 3.1786 1.2549 sec/batch\n",
      "Epoch 1/10  Iteration 118/1780 Training loss: 3.1766 1.2869 sec/batch\n",
      "Epoch 1/10  Iteration 119/1780 Training loss: 3.1748 1.2855 sec/batch\n",
      "Epoch 1/10  Iteration 120/1780 Training loss: 3.1727 1.3075 sec/batch\n",
      "Epoch 1/10  Iteration 121/1780 Training loss: 3.1707 1.3160 sec/batch\n",
      "Epoch 1/10  Iteration 122/1780 Training loss: 3.1687 1.3690 sec/batch\n",
      "Epoch 1/10  Iteration 123/1780 Training loss: 3.1665 1.2442 sec/batch\n",
      "Epoch 1/10  Iteration 124/1780 Training loss: 3.1645 1.2506 sec/batch\n",
      "Epoch 1/10  Iteration 125/1780 Training loss: 3.1622 1.2439 sec/batch\n",
      "Epoch 1/10  Iteration 126/1780 Training loss: 3.1597 1.2924 sec/batch\n",
      "Epoch 1/10  Iteration 127/1780 Training loss: 3.1575 1.2570 sec/batch\n",
      "Epoch 1/10  Iteration 128/1780 Training loss: 3.1553 1.2452 sec/batch\n",
      "Epoch 1/10  Iteration 129/1780 Training loss: 3.1528 1.2501 sec/batch\n",
      "Epoch 1/10  Iteration 130/1780 Training loss: 3.1504 1.2664 sec/batch\n",
      "Epoch 1/10  Iteration 131/1780 Training loss: 3.1481 1.2760 sec/batch\n",
      "Epoch 1/10  Iteration 132/1780 Training loss: 3.1455 1.2627 sec/batch\n",
      "Epoch 1/10  Iteration 133/1780 Training loss: 3.1430 1.2663 sec/batch\n",
      "Epoch 1/10  Iteration 134/1780 Training loss: 3.1405 1.2814 sec/batch\n",
      "Epoch 1/10  Iteration 135/1780 Training loss: 3.1377 1.3052 sec/batch\n",
      "Epoch 1/10  Iteration 136/1780 Training loss: 3.1350 1.2743 sec/batch\n",
      "Epoch 1/10  Iteration 137/1780 Training loss: 3.1324 1.2748 sec/batch\n",
      "Epoch 1/10  Iteration 138/1780 Training loss: 3.1297 1.2983 sec/batch\n",
      "Epoch 1/10  Iteration 139/1780 Training loss: 3.1271 1.3046 sec/batch\n",
      "Epoch 1/10  Iteration 140/1780 Training loss: 3.1245 1.2972 sec/batch\n",
      "Epoch 1/10  Iteration 141/1780 Training loss: 3.1218 1.2828 sec/batch\n",
      "Epoch 1/10  Iteration 142/1780 Training loss: 3.1191 1.2909 sec/batch\n",
      "Epoch 1/10  Iteration 143/1780 Training loss: 3.1163 1.2778 sec/batch\n",
      "Epoch 1/10  Iteration 144/1780 Training loss: 3.1134 1.2905 sec/batch\n",
      "Epoch 1/10  Iteration 145/1780 Training loss: 3.1106 1.2853 sec/batch\n",
      "Epoch 1/10  Iteration 146/1780 Training loss: 3.1079 1.2855 sec/batch\n",
      "Epoch 1/10  Iteration 147/1780 Training loss: 3.1050 1.2803 sec/batch\n",
      "Epoch 1/10  Iteration 148/1780 Training loss: 3.1024 1.2850 sec/batch\n",
      "Epoch 1/10  Iteration 149/1780 Training loss: 3.0994 1.2937 sec/batch\n",
      "Epoch 1/10  Iteration 150/1780 Training loss: 3.0964 1.2770 sec/batch\n",
      "Epoch 1/10  Iteration 151/1780 Training loss: 3.0937 1.2801 sec/batch\n",
      "Epoch 1/10  Iteration 152/1780 Training loss: 3.0911 1.2794 sec/batch\n",
      "Epoch 1/10  Iteration 153/1780 Training loss: 3.0882 1.2705 sec/batch\n",
      "Epoch 1/10  Iteration 154/1780 Training loss: 3.0854 1.2794 sec/batch\n",
      "Epoch 1/10  Iteration 155/1780 Training loss: 3.0824 1.3010 sec/batch\n",
      "Epoch 1/10  Iteration 156/1780 Training loss: 3.0794 1.2899 sec/batch\n",
      "Epoch 1/10  Iteration 157/1780 Training loss: 3.0763 1.2763 sec/batch\n",
      "Epoch 1/10  Iteration 158/1780 Training loss: 3.0732 1.4113 sec/batch\n",
      "Epoch 1/10  Iteration 159/1780 Training loss: 3.0701 1.4950 sec/batch\n",
      "Epoch 1/10  Iteration 160/1780 Training loss: 3.0672 1.4153 sec/batch\n",
      "Epoch 1/10  Iteration 161/1780 Training loss: 3.0642 1.3246 sec/batch\n",
      "Epoch 1/10  Iteration 162/1780 Training loss: 3.0610 1.3455 sec/batch\n",
      "Epoch 1/10  Iteration 163/1780 Training loss: 3.0578 1.2728 sec/batch\n",
      "Epoch 1/10  Iteration 164/1780 Training loss: 3.0547 1.2856 sec/batch\n",
      "Epoch 1/10  Iteration 165/1780 Training loss: 3.0517 1.2867 sec/batch\n",
      "Epoch 1/10  Iteration 166/1780 Training loss: 3.0486 1.2713 sec/batch\n",
      "Epoch 1/10  Iteration 167/1780 Training loss: 3.0456 1.2594 sec/batch\n",
      "Epoch 1/10  Iteration 168/1780 Training loss: 3.0426 1.2958 sec/batch\n",
      "Epoch 1/10  Iteration 169/1780 Training loss: 3.0396 1.2741 sec/batch\n",
      "Epoch 1/10  Iteration 170/1780 Training loss: 3.0365 1.2729 sec/batch\n",
      "Epoch 1/10  Iteration 171/1780 Training loss: 3.0335 1.2771 sec/batch\n",
      "Epoch 1/10  Iteration 172/1780 Training loss: 3.0306 1.2786 sec/batch\n",
      "Epoch 1/10  Iteration 173/1780 Training loss: 3.0279 1.2798 sec/batch\n",
      "Epoch 1/10  Iteration 174/1780 Training loss: 3.0251 1.2815 sec/batch\n",
      "Epoch 1/10  Iteration 175/1780 Training loss: 3.0223 1.2952 sec/batch\n",
      "Epoch 1/10  Iteration 176/1780 Training loss: 3.0193 1.2745 sec/batch\n",
      "Epoch 1/10  Iteration 177/1780 Training loss: 3.0164 1.3026 sec/batch\n",
      "Epoch 1/10  Iteration 178/1780 Training loss: 3.0133 1.2847 sec/batch\n",
      "Epoch 2/10  Iteration 179/1780 Training loss: 2.5470 1.2860 sec/batch\n",
      "Epoch 2/10  Iteration 180/1780 Training loss: 2.5017 1.2805 sec/batch\n",
      "Epoch 2/10  Iteration 181/1780 Training loss: 2.4893 1.2769 sec/batch\n",
      "Epoch 2/10  Iteration 182/1780 Training loss: 2.4845 1.2809 sec/batch\n",
      "Epoch 2/10  Iteration 183/1780 Training loss: 2.4819 1.2980 sec/batch\n",
      "Epoch 2/10  Iteration 184/1780 Training loss: 2.4773 1.2984 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/anna20.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 0.5,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/anna/i{}_l{}_{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/anna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    prime = \"Far\"\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i3560_l512_1.122.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i200_l512_2.432.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i600_l512_1.750.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i1000_l512_1.484.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
